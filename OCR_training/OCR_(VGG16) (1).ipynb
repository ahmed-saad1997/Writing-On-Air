{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kcif8BL3erLx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyWLgz5zu165",
        "outputId": "1e62102a-e439-48b4-ecf5-81f285aa44ec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "H0G8_wk2iRZh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d nibinv23/iam-handwriting-word-database"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc-Hw1k9ijih",
        "outputId": "5d33ca92-0a82-4630-f8c3-f395e1aa41eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading iam-handwriting-word-database.zip to /content\n",
            " 99% 1.10G/1.10G [00:16<00:00, 52.1MB/s]\n",
            "100% 1.10G/1.10G [00:16<00:00, 71.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q iam-handwriting-word-database.zip"
      ],
      "metadata": {
        "id": "R71ukkypivXo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "img_paths=[]\n",
        "for i in os.listdir('iam_words/words'):\n",
        "  for j in os.listdir('iam_words/words/'+i):\n",
        "    for x in os.listdir('iam_words/words/'+i+'/'+j):\n",
        "      img_paths.append('iam_words/words/'+i+'/'+j+'/'+x)\n",
        "len(img_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oisuj5Ri1kx",
        "outputId": "c3b13b8f-bce1-432f-db37-2eab5b151f97"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "115320"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs_labels={}\n",
        "with open('words_new.txt','r') as labels_txt:\n",
        "  lines=labels_txt.readlines()\n",
        "  for i in range(18,len(lines)):\n",
        "    if lines[i].split(' ')[1].strip()!='ok' or len(lines[i].split(' ')[-1].strip())<=1:\n",
        "      continue\n",
        "    img_name=lines[i].strip().split(' ')[0].strip()\n",
        "    label=lines[i].split(' ')[-1].strip()\n",
        "    imgs_labels[img_name]=label\n",
        "    if i==20:\n",
        "      print('`'+img_name+'`','`'+label+'`')\n",
        "\n",
        "len(imgs_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eH6ddpYkAqp",
        "outputId": "fa1c7b11-677f-475e-81cc-57f6929f190f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`a01-000u-00-02` `to`\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33273"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab=[chr(a) for a in range(ord('a'),ord('z')+1)]+[chr(a) for a in range(ord('A'),ord('Z')+1)]\n",
        "for i in imgs_labels:\n",
        "  for x in imgs_labels[i]:\n",
        "    if x not in vocab:\n",
        "      vocab.append(x)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8xz-TCxoioP",
        "outputId": "d03422c0-562b-45a6-e1d4-d65a3abca307"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '.', '-', \"'\", '1', '9', '5', '8', '3', '4', '0', ',', '2', '7', '6', '/', '*', '?', '\"']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir preproc_img"
      ],
      "metadata": {
        "id": "_OKBGnGgqWwe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "def img_reshape(img,name):\n",
        "  shape=(32,128)\n",
        "  target = np.ones(shape)*255\n",
        "  H, W = shape\n",
        "  h, w = img.shape\n",
        "  fx = H/h\n",
        "  fy = W/w\n",
        "  f = min(fx, fy)\n",
        "  _h = int(h*f)\n",
        "  _w = int(w*f)\n",
        "  _img = cv2.resize(img, (_w,_h))\n",
        "  target[:_h,:_w] = _img\n",
        "  cv2.imwrite(name,target)\n",
        "i=0\n",
        "for im in img_paths:\n",
        "  img_name=im.split('/')[-1].strip().split('.')[0]\n",
        "  if img_name in imgs_labels:\n",
        "    img=cv2.imread(im)\n",
        "    try:\n",
        "      if len(img.shape)==3:\n",
        "        img=cv2.imread(im,0)\n",
        "      img_reshape(img,'preproc_img/'+imgs_labels[img_name]+'@IAM'+str(i)+'.png')\n",
        "      i+=1\n",
        "    except:\n",
        "      print(im)\n",
        "len(os.listdir('preproc_img'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BreLzt-wmDMr",
        "outputId": "ba807b8e-10c9-4ba4-f382-03cc85640ff9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iam_words/words/a01/a01-117/a01-117-05-02.png\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33263"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "disnNsdYDqaQ",
        "outputId": "d6f5b519-c8d6-4de0-9af3-b1f4668f08c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-03 14:52:41--  https://www.dropbox.com/s/l2ul3upj7dkv4ou/synthetic-data.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6022:18::a27d:4212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/l2ul3upj7dkv4ou/synthetic-data.zip [following]\n",
            "--2023-09-03 14:52:42--  https://www.dropbox.com/s/raw/l2ul3upj7dkv4ou/synthetic-data.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc6219d93d651d9d878a4f0cce43.dl.dropboxusercontent.com/cd/0/inline/CDAklu8cEIFYaVLmbFnb-IG6AsbkiikkwR2AIx5Hkl4Pnc5P-woDCuSQl9Rs_IMv-YojLLUvOJoJ4z0XAeko6QDd5p9sW1YKRka6Yj2HHQ8-ANZWAnevp6xSUWCcbL8MUrpHCY9WgpB1phhGApb_ae3x/file# [following]\n",
            "--2023-09-03 14:52:43--  https://uc6219d93d651d9d878a4f0cce43.dl.dropboxusercontent.com/cd/0/inline/CDAklu8cEIFYaVLmbFnb-IG6AsbkiikkwR2AIx5Hkl4Pnc5P-woDCuSQl9Rs_IMv-YojLLUvOJoJ4z0XAeko6QDd5p9sW1YKRka6Yj2HHQ8-ANZWAnevp6xSUWCcbL8MUrpHCY9WgpB1phhGApb_ae3x/file\n",
            "Resolving uc6219d93d651d9d878a4f0cce43.dl.dropboxusercontent.com (uc6219d93d651d9d878a4f0cce43.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6022:15::a27d:420f\n",
            "Connecting to uc6219d93d651d9d878a4f0cce43.dl.dropboxusercontent.com (uc6219d93d651d9d878a4f0cce43.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CDBHnm2rdE5xLqe8iPR-6hNY6zASc7zxKYDbnbEyb3h3eb4567p43i89R-WcSafx4Zrwlzl5l9W22X9ECSXCDVIEKEEoWez9PygmxSFxhKSIQmCvdADbv6ulXKekzKc3Ow2NdsdDlWUHVI62vyKGVPMuzR8DtTXTUSHHYJpKM33bTrVEzsbGmBx60UTAfavOp7uEI5enORwG7cIX3EgO0DtZm9wLqj_40UwSNXb4WFh1nLgqxZ2zlso9VpOPzcBkbk6f2ymQ_kywln7KR4-Gt24wraZM_owf4t6bfRjVQRjYX2aFZPQaZ98evgztlkXu8fGTujUYpDhCsr598IAdnk93J_CeLbc4gBE3VX1y31dgbWdgbZ2q8_Qss_F2WGaTLGc/file [following]\n",
            "--2023-09-03 14:52:43--  https://uc6219d93d651d9d878a4f0cce43.dl.dropboxusercontent.com/cd/0/inline2/CDBHnm2rdE5xLqe8iPR-6hNY6zASc7zxKYDbnbEyb3h3eb4567p43i89R-WcSafx4Zrwlzl5l9W22X9ECSXCDVIEKEEoWez9PygmxSFxhKSIQmCvdADbv6ulXKekzKc3Ow2NdsdDlWUHVI62vyKGVPMuzR8DtTXTUSHHYJpKM33bTrVEzsbGmBx60UTAfavOp7uEI5enORwG7cIX3EgO0DtZm9wLqj_40UwSNXb4WFh1nLgqxZ2zlso9VpOPzcBkbk6f2ymQ_kywln7KR4-Gt24wraZM_owf4t6bfRjVQRjYX2aFZPQaZ98evgztlkXu8fGTujUYpDhCsr598IAdnk93J_CeLbc4gBE3VX1y31dgbWdgbZ2q8_Qss_F2WGaTLGc/file\n",
            "Reusing existing connection to uc6219d93d651d9d878a4f0cce43.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 39876999 (38M) [application/zip]\n",
            "Saving to: ‘synthetic-data.zip’\n",
            "\n",
            "synthetic-data.zip  100%[===================>]  38.03M  95.3MB/s    in 0.4s    \n",
            "\n",
            "2023-09-03 14:52:44 (95.3 MB/s) - ‘synthetic-data.zip’ saved [39876999/39876999]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.dropbox.com/s/l2ul3upj7dkv4ou/synthetic-data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bih4F1eqD7an"
      },
      "outputs": [],
      "source": [
        "!unzip -q synthetic-data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "oZK1xj0emzgV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4f9e7b4-76d1-421e-a857-8628410c5a48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['n', 'e', 'w', 'a', 't', 's', 'r', 'u', 'c', 'u', 'r', 'i', 'd', 'y', 'h', 'l', 'l', 'C', 'o', 'g', 'p', 'p', 'v', 'b', 'f', 'q', 'k', 'm', 'x', 'z', 'M', 'j', 'P', 'A', 'T', 'V', 'D', 'I', 'R']\n",
            "14\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "words=[a.strip().split('@')[0] for a in os.listdir('synthetic-data')]\n",
        "chars=[]\n",
        "max_len=0\n",
        "for word in words:\n",
        "  max_len=len(word) if len(word)>max_len else max_len\n",
        "  chars.extend([ch for ch in word if ch not in chars])\n",
        "print(chars)\n",
        "print(max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1iYD0Sx507ee"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "def img_reshape(img,name):\n",
        "  shape=(32,128)\n",
        "  target = np.ones(shape)*255\n",
        "  H, W = shape\n",
        "  h, w = img.shape\n",
        "  fx = H/h\n",
        "  fy = W/w\n",
        "  f = min(fx, fy)\n",
        "  _h = int(h*f)\n",
        "  _w = int(w*f)\n",
        "  _img = cv2.resize(img, (_w,_h))\n",
        "  target[:_h,:_w] = _img\n",
        "  cv2.imwrite(name,target)\n",
        "for im in os.listdir('synthetic-data'):\n",
        "  img=cv2.imread('synthetic-data/'+im,0)\n",
        "  img_reshape(img,'preproc_img/'+im)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BO1Cn6aKpGnq"
      },
      "outputs": [],
      "source": [
        "!pip install -q editdistance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6F4_cT3HFTZH"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import cv2\n",
        "import editdistance\n",
        "import random\n",
        "class dataset(Dataset):\n",
        "  def __init__(self,data,timestep=32,img_size=(32,128)):\n",
        "    super().__init__()\n",
        "    self.data=data\n",
        "    self.timestep=timestep\n",
        "    self.img_size=img_size\n",
        "    self.vocap=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '.', '-', \"'\", '1', '9', '5', '8', '3', '4', '0', ',', '2', '7', '6', '/', '*', '?', '\"']\n",
        "    self.char_code={x:i+1 for i,x in enumerate(self.vocap)}\n",
        "    self.char_code.update({'-':0})\n",
        "    self.code_char={i+1:x for i,x in enumerate(self.vocap)}\n",
        "    self.code_char.update({0:'-'})\n",
        "  def pad(self,code,max_len):\n",
        "    padded_item = code + [0]*(max_len-len(code))\n",
        "    return padded_item\n",
        "  def sample(self):\n",
        "    return self[random.randint(0,len(self))]\n",
        "  def get_image_label(self,image_name):\n",
        "    return image_name.strip().split('@')[0]\n",
        "  def get_label_code(self,label):\n",
        "    return [self.char_code[x] for x in label]\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  def __getitem__(self, index):\n",
        "    item=self.data[index]\n",
        "    image=cv2.imread('preproc_img/'+item,0)\n",
        "    label=self.get_image_label(item)\n",
        "    return image,label\n",
        "  def collate_fn(self, batch):\n",
        "    images, labels, label_lengths, label_vectors, input_lengths = [], [], [], [], []\n",
        "    for image, label in batch:\n",
        "        images.append(torch.Tensor(image)[None,None])\n",
        "        label_lengths.append(len(label))\n",
        "        labels.append(label)\n",
        "        label_code=self.get_label_code(label)\n",
        "        padded=self.pad(label_code,self.timestep)\n",
        "        label_vectors.append(padded)\n",
        "        input_lengths.append(self.timestep)\n",
        "    images = torch.cat(images).float().to(device)\n",
        "    label_lengths = torch.Tensor(label_lengths).long().to(device)\n",
        "    label_vectors = torch.Tensor(label_vectors).long().to(device)\n",
        "    input_lengths = torch.Tensor(input_lengths).long().to(device)\n",
        "    return images, labels, label_lengths,label_vectors,  input_lengths\n",
        "  def wer(self, preds, labels):\n",
        "      c = 0\n",
        "      for p, l in zip(preds, labels):\n",
        "          c += p.lower().strip() != l.lower().strip()\n",
        "      return round(c/len(preds), 4)\n",
        "  def cer(self, preds, labels):\n",
        "      c, d = [], []\n",
        "      for p, l in zip(preds, labels):\n",
        "          c.append(editdistance.eval(p, l) / len(l))\n",
        "      return round(np.mean(c), 4)\n",
        "  def decode(self, pred):\n",
        "      decoded = \"\"\n",
        "      last = \"\"\n",
        "      pred = pred.cpu().detach().numpy()\n",
        "      for i in range(len(pred)):\n",
        "          k = np.argmax(pred[i])\n",
        "          if k > 0 and self.code_char[k] != last:\n",
        "              last = self.code_char[k]\n",
        "              decoded = decoded + last\n",
        "          elif k > 0 and self.code_char[k] == last:\n",
        "              continue\n",
        "          else:\n",
        "              last = \"\"\n",
        "      return decoded.replace(\" \",\"\")\n",
        "  def evaluate(self, model, ims, labels, lower=False):\n",
        "      model.eval()\n",
        "      preds = model(ims).permute(1,0,2)\n",
        "      preds = [self.decode(pred) for pred in preds]\n",
        "      return {'char-error-rate': self.cer(preds, labels),\n",
        "              'word-error-rate': self.wer(preds, labels),\n",
        "              'char-accuracy' : 1 - self.cer(preds, labels),\n",
        "              'word-accuracy' : 1 - self.wer(preds, labels)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VmIxNq7P5BeR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca39fcaf-075c-4372-8c77-2c08df38c15a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58395\n",
            "46716\n",
            "11679\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "batch=64\n",
        "data=os.listdir('preproc_img')\n",
        "trn,val=train_test_split(data,test_size=0.2,random_state=22)\n",
        "print(len(data))\n",
        "print(len(trn))\n",
        "print(len(val))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import shutil\n",
        "paths= os.listdir('preproc_img')\n",
        "for i in range(5):\n",
        "  pat=paths[random.randint(0,len(paths))]\n",
        "  shutil.copy('preproc_img/'+pat,pat)"
      ],
      "metadata": {
        "id": "UDFEW4bl3VXQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nuAmTy-eSza7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image,ImageOps\n",
        "import numpy as np\n",
        "rotate_transform =transforms.RandomRotation(degrees=5)\n",
        "affine_transform=transforms.RandomAffine(degrees=5, translate=(0.01, 0.01), shear=10)\n",
        "gaussblur_transform=transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n",
        "\n",
        "trans=[rotate_transform,affine_transform,gaussblur_transform]\n",
        "aug_trn=[]\n",
        "aug_val=[]\n",
        "def zoom_out(path,pad,aug_list,ind):\n",
        "    img = Image.open('preproc_img/'+path)\n",
        "    padding = pad\n",
        "    width, height = img.size\n",
        "    new_width = width + 2 * padding\n",
        "    new_height = height + 2 * padding\n",
        "    bordered_img = ImageOps.expand(img, border=padding, fill='white')\n",
        "    img_zoom_out = bordered_img.resize((128, 32))\n",
        "    try:\n",
        "      img_zoom_out.save('preproc_img/'+path.split('@')[0]+'@zoom'+str(pad)+str(ind)+path.split('@')[1], format=path.split('.')[-1])\n",
        "    except:\n",
        "      print(path)\n",
        "    aug_list.append(path.split('@')[0]+'@zoom'+str(pad)+str(ind)+path.split('@')[1])\n",
        "for path in trn:\n",
        "    zoom_out(path,25,aug_trn,1)\n",
        "    zoom_out(path,10,aug_trn,2)\n",
        "for path in val:\n",
        "    zoom_out(path,25,aug_val,3)\n",
        "    zoom_out(path,10,aug_val,4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "WIHzKOTuTbgz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebb16988-bc8f-4954-fe1a-595b013307c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "175185\n",
            "140148\n",
            "35037\n"
          ]
        }
      ],
      "source": [
        "trn+=aug_trn\n",
        "val+=aug_val\n",
        "print(len(os.listdir('preproc_img')))\n",
        "print(len(trn))\n",
        "print(len(val))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1-7lx4EshJBsQMijdmwyZOabEl1smnWCa\n",
        "!gdown 1-2KR7w876BaKaUtuTxfvrvkXnrb8qEuf\n",
        "!gdown 1-5kQYw8CR6qKyoUhwVufJ4985OnZ4L5R"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOzXHGKOvRQp",
        "outputId": "4df1a7dd-031e-4c18-80f8-9978e2d061c0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-7lx4EshJBsQMijdmwyZOabEl1smnWCa\n",
            "To: /content/best.pt\n",
            "100% 74.3M/74.3M [00:01<00:00, 66.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-2KR7w876BaKaUtuTxfvrvkXnrb8qEuf\n",
            "To: /content/val_dataset.data\n",
            "100% 1.11M/1.11M [00:00<00:00, 7.75MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-5kQYw8CR6qKyoUhwVufJ4985OnZ4L5R\n",
            "To: /content/trn_dataset.data\n",
            "100% 4.45M/4.45M [00:00<00:00, 26.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ORXeAm9hixuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42ef03a3-300e-4826-ff8e-9937a771f38f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2189\n",
            "547\n"
          ]
        }
      ],
      "source": [
        "# trn_dataset=dataset(trn)\n",
        "# val_dataset=dataset(val)\n",
        "trn_dataset=torch.load('trn_dataset.data',map_location=device)\n",
        "val_dataset=torch.load('val_dataset.data',map_location=device)\n",
        "trn_loader=DataLoader(trn_dataset,collate_fn=trn_dataset.collate_fn,batch_size=batch,drop_last=True,shuffle=True)\n",
        "val_loader=DataLoader(val_dataset,collate_fn=val_dataset.collate_fn,batch_size=batch,drop_last=True)\n",
        "print(len(trn_loader))\n",
        "print(len(val_loader))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ifeW3Fx6i3V",
        "outputId": "9b0704bc-65b5-472c-c307-fe39ce7d9df8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "gNMhwpamiS7M"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torchvision.models.vgg import vgg16_bn\n",
        "class OCRmodel(nn.Module):\n",
        "  def __init__(self,ks=3,st=1,pad=1,pool=2, drop=0.2,vocab_len=70):\n",
        "    super().__init__()\n",
        "    # self.model=nn.Sequential(\n",
        "    #     nn.Conv2d(1, 128, kernel_size=ks, stride=st, padding=pad),\n",
        "    #     nn.BatchNorm2d(128, momentum=0.3),\n",
        "    #     nn.ReLU(inplace=True),\n",
        "    #     nn.MaxPool2d(pool),\n",
        "    #     nn.Dropout2d(drop),\n",
        "    #     nn.Conv2d(128, 128, kernel_size=ks, stride=st, padding=pad),\n",
        "    #     nn.BatchNorm2d(128, momentum=0.3),\n",
        "    #     nn.ReLU(inplace=True),\n",
        "    #     nn.MaxPool2d(pool),\n",
        "    #     nn.Dropout2d(drop),\n",
        "    #     nn.Conv2d(128, 256, kernel_size=ks, stride=st, padding=pad),\n",
        "    #     nn.BatchNorm2d(256, momentum=0.3),\n",
        "    #     nn.ReLU(inplace=True),\n",
        "    #     nn.MaxPool2d((4,2)),\n",
        "    #     nn.Dropout2d(drop)\n",
        "    # )\n",
        "    self.model=vgg16_bn(pretrained=True).features.to(device)\n",
        "    self.model[0]=nn.Conv2d(1, 64, kernel_size=ks, stride=st, padding=pad)\n",
        "    self.model[-1]=nn.Sequential(nn.Conv2d(512, 256, kernel_size=ks, stride=st, padding=pad),\n",
        "                                 nn.Upsample(scale_factor=(1, 2), mode='nearest'),\n",
        "                                 nn.BatchNorm2d(256, momentum=0.3),\n",
        "                                 nn.ReLU(inplace=True))\n",
        "    self.rnn=nn.Sequential(\n",
        "        nn.LSTM(256, 256, num_layers=2, dropout=0.2, bidirectional=True)\n",
        "    )\n",
        "\n",
        "    self.classification = nn.Sequential(\n",
        "        nn.Linear(512, vocab_len+1),\n",
        "        nn.LogSoftmax(-1),\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    x=self.model(x)\n",
        "    x=x.reshape(-1,256,32)\n",
        "    x=x.permute(2,0,1)\n",
        "    x,ls=self.rnn(x)\n",
        "    x=self.classification(x)\n",
        "    return x\n",
        "def ctc(log_probs, target, input_lengths, target_lengths, blank=0):\n",
        "    loss = nn.CTCLoss(blank=blank, zero_infinity=True)\n",
        "    ctc_loss = loss(log_probs, target, input_lengths, target_lengths)\n",
        "    return ctc_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "C4TlI_tGsi6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64b0f659-5b54-4933-bfcc-8bcde0e87402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/61.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q torch_summary\n",
        "!pip install -q torch_snippets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "JMDIdBfPqEAJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e088dfd-a177-4eac-c2cf-e6c99729910b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /root/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth\n",
            "100%|██████████| 528M/528M [00:03<00:00, 155MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "├─Sequential: 1-1                        [-1, 256, 2, 16]          --\n",
            "|    └─Conv2d: 2-1                       [-1, 64, 32, 128]         640\n",
            "|    └─BatchNorm2d: 2-2                  [-1, 64, 32, 128]         128\n",
            "|    └─ReLU: 2-3                         [-1, 64, 32, 128]         --\n",
            "|    └─Conv2d: 2-4                       [-1, 64, 32, 128]         36,928\n",
            "|    └─BatchNorm2d: 2-5                  [-1, 64, 32, 128]         128\n",
            "|    └─ReLU: 2-6                         [-1, 64, 32, 128]         --\n",
            "|    └─MaxPool2d: 2-7                    [-1, 64, 16, 64]          --\n",
            "|    └─Conv2d: 2-8                       [-1, 128, 16, 64]         73,856\n",
            "|    └─BatchNorm2d: 2-9                  [-1, 128, 16, 64]         256\n",
            "|    └─ReLU: 2-10                        [-1, 128, 16, 64]         --\n",
            "|    └─Conv2d: 2-11                      [-1, 128, 16, 64]         147,584\n",
            "|    └─BatchNorm2d: 2-12                 [-1, 128, 16, 64]         256\n",
            "|    └─ReLU: 2-13                        [-1, 128, 16, 64]         --\n",
            "|    └─MaxPool2d: 2-14                   [-1, 128, 8, 32]          --\n",
            "|    └─Conv2d: 2-15                      [-1, 256, 8, 32]          295,168\n",
            "|    └─BatchNorm2d: 2-16                 [-1, 256, 8, 32]          512\n",
            "|    └─ReLU: 2-17                        [-1, 256, 8, 32]          --\n",
            "|    └─Conv2d: 2-18                      [-1, 256, 8, 32]          590,080\n",
            "|    └─BatchNorm2d: 2-19                 [-1, 256, 8, 32]          512\n",
            "|    └─ReLU: 2-20                        [-1, 256, 8, 32]          --\n",
            "|    └─Conv2d: 2-21                      [-1, 256, 8, 32]          590,080\n",
            "|    └─BatchNorm2d: 2-22                 [-1, 256, 8, 32]          512\n",
            "|    └─ReLU: 2-23                        [-1, 256, 8, 32]          --\n",
            "|    └─MaxPool2d: 2-24                   [-1, 256, 4, 16]          --\n",
            "|    └─Conv2d: 2-25                      [-1, 512, 4, 16]          1,180,160\n",
            "|    └─BatchNorm2d: 2-26                 [-1, 512, 4, 16]          1,024\n",
            "|    └─ReLU: 2-27                        [-1, 512, 4, 16]          --\n",
            "|    └─Conv2d: 2-28                      [-1, 512, 4, 16]          2,359,808\n",
            "|    └─BatchNorm2d: 2-29                 [-1, 512, 4, 16]          1,024\n",
            "|    └─ReLU: 2-30                        [-1, 512, 4, 16]          --\n",
            "|    └─Conv2d: 2-31                      [-1, 512, 4, 16]          2,359,808\n",
            "|    └─BatchNorm2d: 2-32                 [-1, 512, 4, 16]          1,024\n",
            "|    └─ReLU: 2-33                        [-1, 512, 4, 16]          --\n",
            "|    └─MaxPool2d: 2-34                   [-1, 512, 2, 8]           --\n",
            "|    └─Conv2d: 2-35                      [-1, 512, 2, 8]           2,359,808\n",
            "|    └─BatchNorm2d: 2-36                 [-1, 512, 2, 8]           1,024\n",
            "|    └─ReLU: 2-37                        [-1, 512, 2, 8]           --\n",
            "|    └─Conv2d: 2-38                      [-1, 512, 2, 8]           2,359,808\n",
            "|    └─BatchNorm2d: 2-39                 [-1, 512, 2, 8]           1,024\n",
            "|    └─ReLU: 2-40                        [-1, 512, 2, 8]           --\n",
            "|    └─Conv2d: 2-41                      [-1, 512, 2, 8]           2,359,808\n",
            "|    └─BatchNorm2d: 2-42                 [-1, 512, 2, 8]           1,024\n",
            "|    └─ReLU: 2-43                        [-1, 512, 2, 8]           --\n",
            "|    └─Sequential: 2-44                  [-1, 256, 2, 16]          --\n",
            "|    |    └─Conv2d: 3-1                  [-1, 256, 2, 8]           1,179,904\n",
            "|    |    └─Upsample: 3-2                [-1, 256, 2, 16]          --\n",
            "|    |    └─BatchNorm2d: 3-3             [-1, 256, 2, 16]          512\n",
            "|    |    └─ReLU: 3-4                    [-1, 256, 2, 16]          --\n",
            "├─Sequential: 1-2                        [-1, 1, 512]              --\n",
            "|    └─LSTM: 2-45                        [-1, 1, 512]              2,629,632\n",
            "├─Sequential: 1-3                        [-1, 1, 71]               --\n",
            "|    └─Linear: 2-46                      [-1, 1, 71]               36,423\n",
            "|    └─LogSoftmax: 2-47                  [-1, 1, 71]               --\n",
            "==========================================================================================\n",
            "Total params: 18,568,455\n",
            "Trainable params: 18,568,455\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 1.29\n",
            "==========================================================================================\n",
            "Input size (MB): 0.02\n",
            "Forward/backward pass size (MB): 16.97\n",
            "Params size (MB): 70.83\n",
            "Estimated Total Size (MB): 87.82\n",
            "==========================================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "├─Sequential: 1-1                        [-1, 256, 2, 16]          --\n",
              "|    └─Conv2d: 2-1                       [-1, 64, 32, 128]         640\n",
              "|    └─BatchNorm2d: 2-2                  [-1, 64, 32, 128]         128\n",
              "|    └─ReLU: 2-3                         [-1, 64, 32, 128]         --\n",
              "|    └─Conv2d: 2-4                       [-1, 64, 32, 128]         36,928\n",
              "|    └─BatchNorm2d: 2-5                  [-1, 64, 32, 128]         128\n",
              "|    └─ReLU: 2-6                         [-1, 64, 32, 128]         --\n",
              "|    └─MaxPool2d: 2-7                    [-1, 64, 16, 64]          --\n",
              "|    └─Conv2d: 2-8                       [-1, 128, 16, 64]         73,856\n",
              "|    └─BatchNorm2d: 2-9                  [-1, 128, 16, 64]         256\n",
              "|    └─ReLU: 2-10                        [-1, 128, 16, 64]         --\n",
              "|    └─Conv2d: 2-11                      [-1, 128, 16, 64]         147,584\n",
              "|    └─BatchNorm2d: 2-12                 [-1, 128, 16, 64]         256\n",
              "|    └─ReLU: 2-13                        [-1, 128, 16, 64]         --\n",
              "|    └─MaxPool2d: 2-14                   [-1, 128, 8, 32]          --\n",
              "|    └─Conv2d: 2-15                      [-1, 256, 8, 32]          295,168\n",
              "|    └─BatchNorm2d: 2-16                 [-1, 256, 8, 32]          512\n",
              "|    └─ReLU: 2-17                        [-1, 256, 8, 32]          --\n",
              "|    └─Conv2d: 2-18                      [-1, 256, 8, 32]          590,080\n",
              "|    └─BatchNorm2d: 2-19                 [-1, 256, 8, 32]          512\n",
              "|    └─ReLU: 2-20                        [-1, 256, 8, 32]          --\n",
              "|    └─Conv2d: 2-21                      [-1, 256, 8, 32]          590,080\n",
              "|    └─BatchNorm2d: 2-22                 [-1, 256, 8, 32]          512\n",
              "|    └─ReLU: 2-23                        [-1, 256, 8, 32]          --\n",
              "|    └─MaxPool2d: 2-24                   [-1, 256, 4, 16]          --\n",
              "|    └─Conv2d: 2-25                      [-1, 512, 4, 16]          1,180,160\n",
              "|    └─BatchNorm2d: 2-26                 [-1, 512, 4, 16]          1,024\n",
              "|    └─ReLU: 2-27                        [-1, 512, 4, 16]          --\n",
              "|    └─Conv2d: 2-28                      [-1, 512, 4, 16]          2,359,808\n",
              "|    └─BatchNorm2d: 2-29                 [-1, 512, 4, 16]          1,024\n",
              "|    └─ReLU: 2-30                        [-1, 512, 4, 16]          --\n",
              "|    └─Conv2d: 2-31                      [-1, 512, 4, 16]          2,359,808\n",
              "|    └─BatchNorm2d: 2-32                 [-1, 512, 4, 16]          1,024\n",
              "|    └─ReLU: 2-33                        [-1, 512, 4, 16]          --\n",
              "|    └─MaxPool2d: 2-34                   [-1, 512, 2, 8]           --\n",
              "|    └─Conv2d: 2-35                      [-1, 512, 2, 8]           2,359,808\n",
              "|    └─BatchNorm2d: 2-36                 [-1, 512, 2, 8]           1,024\n",
              "|    └─ReLU: 2-37                        [-1, 512, 2, 8]           --\n",
              "|    └─Conv2d: 2-38                      [-1, 512, 2, 8]           2,359,808\n",
              "|    └─BatchNorm2d: 2-39                 [-1, 512, 2, 8]           1,024\n",
              "|    └─ReLU: 2-40                        [-1, 512, 2, 8]           --\n",
              "|    └─Conv2d: 2-41                      [-1, 512, 2, 8]           2,359,808\n",
              "|    └─BatchNorm2d: 2-42                 [-1, 512, 2, 8]           1,024\n",
              "|    └─ReLU: 2-43                        [-1, 512, 2, 8]           --\n",
              "|    └─Sequential: 2-44                  [-1, 256, 2, 16]          --\n",
              "|    |    └─Conv2d: 3-1                  [-1, 256, 2, 8]           1,179,904\n",
              "|    |    └─Upsample: 3-2                [-1, 256, 2, 16]          --\n",
              "|    |    └─BatchNorm2d: 3-3             [-1, 256, 2, 16]          512\n",
              "|    |    └─ReLU: 3-4                    [-1, 256, 2, 16]          --\n",
              "├─Sequential: 1-2                        [-1, 1, 512]              --\n",
              "|    └─LSTM: 2-45                        [-1, 1, 512]              2,629,632\n",
              "├─Sequential: 1-3                        [-1, 1, 71]               --\n",
              "|    └─Linear: 2-46                      [-1, 1, 71]               36,423\n",
              "|    └─LogSoftmax: 2-47                  [-1, 1, 71]               --\n",
              "==========================================================================================\n",
              "Total params: 18,568,455\n",
              "Trainable params: 18,568,455\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 1.29\n",
              "==========================================================================================\n",
              "Input size (MB): 0.02\n",
              "Forward/backward pass size (MB): 16.97\n",
              "Params size (MB): 70.83\n",
              "Estimated Total Size (MB): 87.82\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "from torch_snippets import Report\n",
        "model=OCRmodel()\n",
        "model.to(device)\n",
        "state_dict=torch.load('best.pt',map_location=device)\n",
        "model.load_state_dict(state_dict=state_dict)\n",
        "summary(model, torch.zeros((1,1,32,128)).to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBr66V3VyM21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4f1d93e-266e-4e39-bec1-06b8a8a91197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 22.000  validation_loss: 0.403  validation_char_acc: 0.897  validation_word_acc: 0.719  (410.39s - 1455.01s remaining)saving best weights -- epoch 21\n",
            "\n",
            "EPOCH: 22.000  validation_word_acc: 0.785  validation_char_acc: 0.917  train_char_acc: 0.966  validation_loss: 0.337  train_word_acc: 0.887  train_loss: 0.113  (410.77s - 1456.37s remaining)\n",
            "\n",
            "Pred: `intersdation` :: Truth: `interrogations`\n",
            "Pred: `Ralh` :: Truth: `Rabh`\n",
            "Pred: `have` :: Truth: `have`\n",
            "Pred: `leave` :: Truth: `leave`\n",
            "Pred: `industry` :: Truth: `industry`\n",
            "test img: Fie\n",
            "\n",
            "EPOCH: 23.000  validation_loss: 0.348  validation_char_acc: 0.914  validation_word_acc: 0.781  (822.41s - 2753.29s remaining)saving best weights -- epoch 22\n",
            "\n",
            "EPOCH: 23.000  validation_word_acc: 0.795  validation_char_acc: 0.921  train_char_acc: 0.973  validation_loss: 0.335  train_word_acc: 0.901  train_loss: 0.105  (822.98s - 2755.18s remaining)\n",
            "\n",
            "Pred: `recognize` :: Truth: `recognize`\n",
            "Pred: `not` :: Truth: `not`\n",
            "Pred: `few` :: Truth: `few`\n",
            "Pred: `enegy` :: Truth: `energy`\n",
            "Pred: `later` :: Truth: `later`\n",
            "test img: pie\n",
            "\n",
            "EPOCH: 24.000  validation_word_acc: 0.778  validation_char_acc: 0.912  train_char_acc: 0.967  validation_loss: 0.371  train_word_acc: 0.891  train_loss: 0.105  (1234.35s - 3908.77s remaining)\n",
            "\n",
            "Pred: `person` :: Truth: `person`\n",
            "Pred: `line` :: Truth: `line`\n",
            "Pred: `mamufoctuve` :: Truth: `manufacture`\n",
            "Pred: `coach` :: Truth: `coach`\n",
            "Pred: `Mr.` :: Truth: `Mr.`\n",
            "test img: plew\n",
            "\n",
            "EPOCH: 25.000  validation_word_acc: 0.780  validation_char_acc: 0.915  train_char_acc: 0.970  validation_loss: 0.366  train_word_acc: 0.897  train_loss: 0.098  (1645.21s - 4935.63s remaining)\n",
            "\n",
            "Pred: `man` :: Truth: `man`\n",
            "Pred: `nuclear` :: Truth: `nuclear`\n",
            "Pred: `plan` :: Truth: `plan`\n",
            "Pred: `womder` :: Truth: `wonder`\n",
            "Pred: `them` :: Truth: `them`\n",
            "test img: pie\n",
            "\n",
            "EPOCH: 26.000  validation_word_acc: 0.753  validation_char_acc: 0.891  train_char_acc: 0.970  validation_loss: 0.485  train_word_acc: 0.900  train_loss: 0.097  (2056.69s - 5853.66s remaining)\n",
            "\n",
            "Pred: `the` :: Truth: `the`\n",
            "Pred: `entire` :: Truth: `entire`\n",
            "Pred: `position` :: Truth: `position`\n",
            "Pred: `different` :: Truth: `different`\n",
            "Pred: `little` :: Truth: `little`\n",
            "test img: pie\n",
            "\n",
            "EPOCH: 27.000  validation_loss: 0.298  validation_char_acc: 0.931  validation_word_acc: 0.797  (2468.49s - 6674.08s remaining)saving best weights -- epoch 26\n",
            "\n",
            "EPOCH: 27.000  validation_word_acc: 0.802  validation_char_acc: 0.926  train_char_acc: 0.972  validation_loss: 0.318  train_word_acc: 0.906  train_loss: 0.094  (2468.96s - 6675.33s remaining)\n",
            "\n",
            "Pred: `year` :: Truth: `year`\n",
            "Pred: `simple` :: Truth: `simple`\n",
            "Pred: `defended` :: Truth: `defended`\n",
            "Pred: `one` :: Truth: `one`\n",
            "Pred: `fist` :: Truth: `last`\n",
            "test img: pien\n",
            "\n",
            "EPOCH: 28.000  validation_loss: 0.471  validation_char_acc: 0.908  validation_word_acc: 0.688  (2879.72s - 7404.98s remaining)saving best weights -- epoch 27\n",
            "\n",
            "EPOCH: 28.000  validation_word_acc: 0.805  validation_char_acc: 0.928  train_char_acc: 0.972  validation_loss: 0.315  train_word_acc: 0.905  train_loss: 0.093  (2880.13s - 7406.04s remaining)\n",
            "\n",
            "Pred: `relate` :: Truth: `relate`\n",
            "Pred: `first` :: Truth: `first`\n",
            "Pred: `to` :: Truth: `to`\n",
            "Pred: `Book` :: Truth: `Book`\n",
            "Pred: `Levi` :: Truth: `Levi`\n",
            "test img: pice\n",
            "\n",
            "EPOCH: 29.000  validation_word_acc: 0.803  validation_char_acc: 0.926  train_char_acc: 0.971  validation_loss: 0.321  train_word_acc: 0.903  train_loss: 0.092  (3291.16s - 8057.67s remaining)\n",
            "\n",
            "Pred: `stuff` :: Truth: `stuff`\n",
            "Pred: `level` :: Truth: `level`\n",
            "Pred: `on` :: Truth: `an`\n",
            "Pred: `seen` :: Truth: `seen`\n",
            "Pred: `down` :: Truth: `down`\n",
            "test img: ple\n",
            "\n",
            "EPOCH: 30.000  validation_word_acc: 0.783  validation_char_acc: 0.917  train_char_acc: 0.970  validation_loss: 0.362  train_word_acc: 0.902  train_loss: 0.092  (3701.39s - 8636.57s remaining)\n",
            "\n",
            "Pred: `the` :: Truth: `the`\n",
            "Pred: `move` :: Truth: `move`\n",
            "Pred: `said` :: Truth: `said`\n",
            "Pred: `such` :: Truth: `such`\n",
            "Pred: `is` :: Truth: `is`\n",
            "test img: piea\n",
            "\n",
            "EPOCH: 31.000  validation_word_acc: 0.786  validation_char_acc: 0.918  train_char_acc: 0.970  validation_loss: 0.348  train_word_acc: 0.901  train_loss: 0.090  (4111.46s - 9151.32s remaining)\n",
            "\n",
            "Pred: `ther` :: Truth: `their`\n",
            "Pred: `choose` :: Truth: `choose`\n",
            "Pred: `to` :: Truth: `to`\n",
            "Pred: `apply` :: Truth: `apply`\n",
            "Pred: `nation` :: Truth: `nation`\n",
            "test img: pice\n",
            "\n",
            "EPOCH: 32.000  validation_loss: 0.378  validation_char_acc: 0.920  validation_word_acc: 0.797  (4521.14s - 9607.43s remaining)saving best weights -- epoch 31\n",
            "\n",
            "EPOCH: 32.000  validation_word_acc: 0.812  validation_char_acc: 0.930  train_char_acc: 0.976  validation_loss: 0.304  train_word_acc: 0.915  train_loss: 0.089  (4521.58s - 9608.36s remaining)\n",
            "\n",
            "Pred: `this` :: Truth: `this`\n",
            "Pred: `Bife` :: Truth: `life`\n",
            "Pred: `animal` :: Truth: `animal`\n",
            "Pred: `probably` :: Truth: `probably`\n",
            "Pred: `is` :: Truth: `is`\n",
            "test img: pien\n",
            "\n",
            "EPOCH: 33.000  validation_word_acc: 0.804  validation_char_acc: 0.926  train_char_acc: 0.973  validation_loss: 0.323  train_word_acc: 0.910  train_loss: 0.085  (4929.40s - 10008.17s remaining)\n",
            "\n",
            "Pred: `saints` :: Truth: `saints`\n",
            "Pred: `role` :: Truth: `role`\n",
            "Pred: `been` :: Truth: `been`\n",
            "Pred: `never` :: Truth: `never`\n",
            "Pred: `north` :: Truth: `north`\n",
            "test img: fivem\n",
            "\n",
            "EPOCH: 34.000  validation_word_acc: 0.796  validation_char_acc: 0.923  train_char_acc: 0.976  validation_loss: 0.337  train_word_acc: 0.915  train_loss: 0.086  (5336.59s - 10359.27s remaining)\n",
            "\n",
            "Pred: `early` :: Truth: `early`\n",
            "Pred: `is` :: Truth: `is`\n",
            "Pred: `list` :: Truth: `list`\n",
            "Pred: `haove` :: Truth: `have`\n",
            "Pred: `side` :: Truth: `side`\n",
            "test img: pien\n",
            "\n",
            "EPOCH: 35.000  validation_word_acc: 0.805  validation_char_acc: 0.926  train_char_acc: 0.977  validation_loss: 0.315  train_word_acc: 0.916  train_loss: 0.087  (5742.73s - 10665.07s remaining)\n",
            "\n",
            "Pred: `in` :: Truth: `in`\n",
            "Pred: `Washington` :: Truth: `Washington`\n",
            "Pred: `reading` :: Truth: `reading`\n",
            "Pred: `TIe` :: Truth: `An`\n",
            "Pred: `party` :: Truth: `party`\n",
            "test img: pie\n",
            "\n",
            "EPOCH: 36.000  validation_word_acc: 0.751  validation_char_acc: 0.899  train_char_acc: 0.973  validation_loss: 0.457  train_word_acc: 0.911  train_loss: 0.083  (6150.00s - 10933.33s remaining)\n",
            "\n",
            "Pred: `to` :: Truth: `to`\n",
            "Pred: `kid` :: Truth: `kid`\n",
            "Pred: `They` :: Truth: `They`\n",
            "Pred: `say` :: Truth: `say`\n",
            "Pred: `Congress` :: Truth: `Congress`\n",
            "test img: pie\n",
            "\n",
            "EPOCH: 37.000  validation_word_acc: 0.777  validation_char_acc: 0.912  train_char_acc: 0.981  validation_loss: 0.371  train_word_acc: 0.927  train_loss: 0.080  (6556.68s - 11164.07s remaining)\n",
            "\n",
            "Pred: `while` :: Truth: `whole`\n",
            "Pred: `books` :: Truth: `books`\n",
            "Pred: `do` :: Truth: `do`\n",
            "Pred: `the` :: Truth: `the`\n",
            "Pred: `they` :: Truth: `they`\n",
            "test img: pie\n",
            "\n",
            "EPOCH: 38.000  validation_word_acc: 0.788  validation_char_acc: 0.920  train_char_acc: 0.977  validation_loss: 0.357  train_word_acc: 0.916  train_loss: 0.084  (6963.51s - 11361.52s remaining)\n",
            "\n",
            "Pred: `resign` :: Truth: `reign`\n",
            "Pred: `certavnlly` :: Truth: `certainly`\n",
            "Pred: `gun` :: Truth: `gun`\n",
            "Pred: `neightorm` :: Truth: `neighbour`\n",
            "Pred: `shoulder` :: Truth: `shoulder`\n",
            "test img: pies\n",
            "\n",
            "EPOCH: 39.000  validation_word_acc: 0.771  validation_char_acc: 0.909  train_char_acc: 0.977  validation_loss: 0.395  train_word_acc: 0.919  train_loss: 0.078  (7370.84s - 11528.75s remaining)\n",
            "\n",
            "Pred: `during` :: Truth: `during`\n",
            "Pred: `and` :: Truth: `and`\n",
            "Pred: `attempt` :: Truth: `attempt`\n",
            "Pred: `in` :: Truth: `in`\n",
            "Pred: `obvions` :: Truth: `obvious`\n",
            "test img: Piy\n",
            "\n",
            "EPOCH: 40.000  validation_word_acc: 0.801  validation_char_acc: 0.924  train_char_acc: 0.978  validation_loss: 0.334  train_word_acc: 0.921  train_loss: 0.079  (7778.57s - 11667.86s remaining)\n",
            "\n",
            "Pred: `the` :: Truth: `the`\n",
            "Pred: `other` :: Truth: `other`\n",
            "Pred: `almost` :: Truth: `almost`\n",
            "Pred: `coave` :: Truth: `cease`\n",
            "Pred: `government` :: Truth: `Government`\n",
            "test img: plien\n",
            "\n",
            "EPOCH: 41.000  validation_word_acc: 0.808  validation_char_acc: 0.927  train_char_acc: 0.978  validation_loss: 0.328  train_word_acc: 0.921  train_loss: 0.077  (8186.70s - 11780.85s remaining)\n",
            "\n",
            "Pred: `from` :: Truth: `from`\n",
            "Pred: `become` :: Truth: `become`\n",
            "Pred: `this` :: Truth: `this`\n",
            "Pred: `sound` :: Truth: `sound`\n",
            "Pred: `in` :: Truth: `in`\n",
            "test img: plec\n",
            "\n",
            "EPOCH: 42.000  validation_word_acc: 0.776  validation_char_acc: 0.911  train_char_acc: 0.976  validation_loss: 0.384  train_word_acc: 0.916  train_loss: 0.080  (8593.73s - 11867.53s remaining)\n",
            "\n",
            "Pred: `large` :: Truth: `large`\n",
            "Pred: `customer` :: Truth: `customer`\n",
            "Pred: `change` :: Truth: `change`\n",
            "Pred: `reveal` :: Truth: `reveal`\n",
            "Pred: `always` :: Truth: `always`\n",
            "test img: fle\n",
            "\n",
            "EPOCH: 43.000  validation_word_acc: 0.811  validation_char_acc: 0.930  train_char_acc: 0.966  validation_loss: 0.301  train_word_acc: 0.902  train_loss: 0.075  (9001.55s - 11932.29s remaining)\n",
            "\n",
            "Pred: `was` :: Truth: `was`\n",
            "Pred: `effective` :: Truth: `effective`\n",
            "Pred: `firm` :: Truth: `from`\n",
            "Pred: `these` :: Truth: `there`\n",
            "Pred: `resource` :: Truth: `resource`\n",
            "test img: ples\n",
            "\n",
            "EPOCH: 44.000  validation_word_acc: 0.811  validation_char_acc: 0.928  train_char_acc: 0.975  validation_loss: 0.319  train_word_acc: 0.919  train_loss: 0.076  (9410.72s - 11977.28s remaining)\n",
            "\n",
            "Pred: `any` :: Truth: `any`\n",
            "Pred: `atomic` :: Truth: `atomic`\n",
            "Pred: `arrive` :: Truth: `arrive`\n",
            "Pred: `something` :: Truth: `something`\n",
            "Pred: `Newumarhet` :: Truth: `Newmarket`\n",
            "test img: pies\n",
            "\n",
            "EPOCH: 45.000  validation_word_acc: 0.811  validation_char_acc: 0.929  train_char_acc: 0.961  validation_loss: 0.316  train_word_acc: 0.893  train_loss: 0.074  (9819.50s - 12001.61s remaining)\n",
            "\n",
            "Pred: `had` :: Truth: `had`\n",
            "Pred: `over` :: Truth: `over`\n",
            "Pred: `it` :: Truth: `it`\n",
            "Pred: `more` :: Truth: `more`\n",
            "Pred: `will` :: Truth: `will`\n",
            "test img: pie\n",
            "\n",
            "EPOCH: 46.000  validation_loss: 0.464  validation_char_acc: 0.901  validation_word_acc: 0.734  (10228.62s - 12007.51s remaining)saving best weights -- epoch 45\n",
            "\n",
            "EPOCH: 46.000  validation_word_acc: 0.814  validation_char_acc: 0.929  train_char_acc: 0.971  validation_loss: 0.305  train_word_acc: 0.908  train_loss: 0.075  (10229.25s - 12008.25s remaining)\n",
            "\n",
            "Pred: `stock` :: Truth: `stock`\n",
            "Pred: `clearent` :: Truth: `clear-cut`\n",
            "Pred: `policy` :: Truth: `policy`\n",
            "Pred: `at` :: Truth: `at`\n",
            "Pred: `white` :: Truth: `white`\n",
            "test img: pies\n",
            "\n",
            "EPOCH: 47.000  validation_word_acc: 0.808  validation_char_acc: 0.926  train_char_acc: 0.972  validation_loss: 0.332  train_word_acc: 0.910  train_loss: 0.076  (10637.15s - 11995.08s remaining)\n",
            "\n",
            "Pred: `every` :: Truth: `every`\n",
            "Pred: `modern` :: Truth: `modern`\n",
            "Pred: `actually` :: Truth: `actually`\n",
            "Pred: `puh` :: Truth: `put`\n",
            "Pred: `bases` :: Truth: `bases`\n",
            "test img: piew\n",
            "\n",
            "EPOCH: 48.000  validation_word_acc: 0.800  validation_char_acc: 0.926  train_char_acc: 0.967  validation_loss: 0.319  train_word_acc: 0.903  train_loss: 0.073  (11047.63s - 11968.27s remaining)\n",
            "\n",
            "Pred: `delegates` :: Truth: `delegates`\n",
            "Pred: `reason` :: Truth: `reason`\n",
            "Pred: `comfortable` :: Truth: `comfortable`\n",
            "Pred: `'s` :: Truth: `'s`\n",
            "Pred: `Souter` :: Truth: `Souter`\n",
            "test img: pien\n",
            "\n",
            "EPOCH: 49.000  validation_word_acc: 0.807  validation_char_acc: 0.928  train_char_acc: 0.975  validation_loss: 0.304  train_word_acc: 0.918  train_loss: 0.074  (11459.52s - 11927.26s remaining)\n",
            "\n",
            "Pred: `rest` :: Truth: `rest`\n",
            "Pred: `how` :: Truth: `how`\n",
            "Pred: `approach` :: Truth: `approach`\n",
            "Pred: `best` :: Truth: `best`\n",
            "Pred: `on` :: Truth: `on`\n",
            "test img: pie\n",
            "\n",
            "EPOCH: 50.000  validation_loss: 0.381  validation_char_acc: 0.918  validation_word_acc: 0.766  (11869.72s - 11869.72s remaining)saving best weights -- epoch 49\n",
            "\n",
            "EPOCH: 50.000  validation_word_acc: 0.823  validation_char_acc: 0.933  train_char_acc: 0.981  validation_loss: 0.294  train_word_acc: 0.932  train_loss: 0.070  (11870.26s - 11870.26s remaining)\n",
            "\n",
            "Pred: `page` :: Truth: `page`\n",
            "Pred: `had` :: Truth: `had`\n",
            "Pred: `work` :: Truth: `work`\n",
            "Pred: `water` :: Truth: `water`\n",
            "Pred: `pass` :: Truth: `pass`\n",
            "test img: Rie\n",
            "\n",
            "EPOCH: 51.000  validation_loss: 0.318  validation_char_acc: 0.950  validation_word_acc: 0.844  (12279.38s - 11797.84s remaining)saving best weights -- epoch 50\n",
            "\n",
            "EPOCH: 51.000  validation_word_acc: 0.825  validation_char_acc: 0.935  train_char_acc: 0.979  validation_loss: 0.297  train_word_acc: 0.927  train_loss: 0.073  (12279.91s - 11798.34s remaining)\n",
            "\n",
            "Pred: `solid` :: Truth: `solid`\n",
            "Pred: `the` :: Truth: `the`\n",
            "Pred: `the` :: Truth: `the`\n",
            "Pred: `ere` :: Truth: `eve`\n",
            "Pred: `direct` :: Truth: `direct`\n",
            "test img: pler\n",
            "\n",
            "EPOCH: 52.000  validation_word_acc: 0.799  validation_char_acc: 0.924  train_char_acc: 0.980  validation_loss: 0.346  train_word_acc: 0.926  train_loss: 0.075  (12688.36s - 11712.33s remaining)\n",
            "\n",
            "Pred: `urgent` :: Truth: `urgent`\n",
            "Pred: `jonerey` :: Truth: `generals`\n",
            "Pred: `show` :: Truth: `show`\n",
            "Pred: `go` :: Truth: `go`\n",
            "Pred: `dubl` :: Truth: `dull`\n",
            "test img: fier\n",
            "\n",
            "EPOCH: 53.000  validation_loss: 0.393  validation_char_acc: 0.913  validation_word_acc: 0.750  (13096.17s - 11613.58s remaining)saving best weights -- epoch 52\n",
            "\n",
            "EPOCH: 53.000  validation_word_acc: 0.834  validation_char_acc: 0.938  train_char_acc: 0.981  validation_loss: 0.279  train_word_acc: 0.932  train_loss: 0.069  (13096.65s - 11614.01s remaining)\n",
            "\n",
            "Pred: `the` :: Truth: `the`\n",
            "Pred: `innocnt` :: Truth: `innocent`\n",
            "Pred: `could` :: Truth: `could`\n",
            "Pred: `should` :: Truth: `should`\n",
            "Pred: `four` :: Truth: `four`\n",
            "test img: piec\n",
            "\n",
            "EPOCH: 54.000  validation_word_acc: 0.814  validation_char_acc: 0.930  train_char_acc: 0.974  validation_loss: 0.326  train_word_acc: 0.918  train_loss: 0.068  (13506.28s - 11505.35s remaining)\n",
            "\n",
            "Pred: `house` :: Truth: `house`\n",
            "Pred: `worry` :: Truth: `worry`\n",
            "Pred: `manage` :: Truth: `manage`\n",
            "Pred: `that` :: Truth: `that`\n",
            "Pred: `occur` :: Truth: `occur`\n",
            "test img: fier\n",
            "\n",
            "EPOCH: 55.000  validation_word_acc: 0.821  validation_char_acc: 0.932  train_char_acc: 0.975  validation_loss: 0.300  train_word_acc: 0.918  train_loss: 0.069  (13916.61s - 11386.32s remaining)\n",
            "\n",
            "Pred: `thus` :: Truth: `thus`\n",
            "Pred: `is` :: Truth: `is`\n",
            "Pred: `believe` :: Truth: `believe`\n",
            "Pred: `rich` :: Truth: `rich`\n",
            "Pred: `not` :: Truth: `not`\n",
            "test img: piec\n",
            "\n",
            "EPOCH: 56.000  validation_word_acc: 0.697  validation_char_acc: 0.862  train_char_acc: 0.978  validation_loss: 0.559  train_word_acc: 0.923  train_loss: 0.072  (14327.66s - 11257.45s remaining)\n",
            "\n",
            "Pred: `significant` :: Truth: `significant`\n",
            "Pred: `report` :: Truth: `report`\n",
            "Pred: `sat` :: Truth: `seat`\n",
            "Pred: `phone` :: Truth: `phone`\n",
            "Pred: `Frederation` :: Truth: `Federation`\n",
            "test img: Fie\n",
            "\n",
            "EPOCH: 57.000  validation_word_acc: 0.797  validation_char_acc: 0.919  train_char_acc: 0.969  validation_loss: 0.351  train_word_acc: 0.911  train_loss: 0.072  (14737.60s - 11117.84s remaining)\n",
            "\n",
            "Pred: `was` :: Truth: `was`\n",
            "Pred: `members` :: Truth: `member`\n",
            "Pred: `savings` :: Truth: `savings`\n",
            "Pred: `the` :: Truth: `the`\n",
            "Pred: `baby` :: Truth: `baby`\n",
            "test img: Piew\n",
            "\n",
            "EPOCH: 58.000  validation_word_acc: 0.824  validation_char_acc: 0.936  train_char_acc: 0.964  validation_loss: 0.299  train_word_acc: 0.902  train_loss: 0.067  (15147.95s - 10969.21s remaining)\n",
            "\n",
            "Pred: `put` :: Truth: `put`\n",
            "Pred: `even` :: Truth: `even`\n",
            "Pred: `in` :: Truth: `in`\n",
            "Pred: `diference` :: Truth: `difference`\n",
            "Pred: `computer` :: Truth: `computer`\n",
            "test img: pier\n",
            "\n",
            "EPOCH: 59.000  validation_word_acc: 0.808  validation_char_acc: 0.929  train_char_acc: 0.977  validation_loss: 0.326  train_word_acc: 0.926  train_loss: 0.067  (15560.17s - 10813.00s remaining)\n",
            "\n",
            "Pred: `the` :: Truth: `the`\n",
            "Pred: `effort` :: Truth: `effort`\n",
            "Pred: `administration` :: Truth: `administration`\n",
            "Pred: `practice` :: Truth: `practice`\n",
            "Pred: `the` :: Truth: `the`\n",
            "test img: pie\n",
            "\n",
            "EPOCH: 60.000  validation_word_acc: 0.819  validation_char_acc: 0.933  train_char_acc: 0.968  validation_loss: 0.295  train_word_acc: 0.914  train_loss: 0.070  (15970.48s - 10646.99s remaining)\n",
            "\n",
            "Pred: `maybe` :: Truth: `maybe`\n",
            "Pred: `this` :: Truth: `this`\n",
            "Pred: `stand` :: Truth: `stand`\n",
            "Pred: `am` :: Truth: `am`\n",
            "Pred: `in` :: Truth: `in`\n",
            "test img: Pie\n",
            "\n",
            "EPOCH: 61.000  validation_word_acc: 0.824  validation_char_acc: 0.934  train_char_acc: 0.969  validation_loss: 0.302  train_word_acc: 0.910  train_loss: 0.065  (16380.59s - 10472.83s remaining)\n",
            "\n",
            "Pred: `bigin` :: Truth: `begin`\n",
            "Pred: `nearly` :: Truth: `nearly`\n",
            "Pred: `and` :: Truth: `and`\n",
            "Pred: `newspaper` :: Truth: `newspaper`\n",
            "Pred: `fity` :: Truth: `fifty`\n",
            "test img: pien\n",
            "\n",
            "EPOCH: 62.000  validation_word_acc: 0.829  validation_char_acc: 0.935  train_char_acc: 0.976  validation_loss: 0.282  train_word_acc: 0.923  train_loss: 0.070  (16789.81s - 10290.53s remaining)\n",
            "\n",
            "Pred: `four` :: Truth: `four`\n",
            "Pred: `moulds` :: Truth: `moulds`\n",
            "Pred: `money` :: Truth: `money`\n",
            "Pred: `nation` :: Truth: `nation`\n",
            "Pred: `community` :: Truth: `community`\n",
            "test img: Pies\n",
            "\n",
            "EPOCH: 63.000  validation_word_acc: 0.810  validation_char_acc: 0.928  train_char_acc: 0.980  validation_loss: 0.308  train_word_acc: 0.932  train_loss: 0.066  (17200.00s - 10101.59s remaining)\n",
            "\n",
            "Pred: `up` :: Truth: `up`\n",
            "Pred: `the` :: Truth: `the`\n",
            "Pred: `Holy` :: Truth: `Holy`\n",
            "Pred: `they` :: Truth: `Key`\n",
            "Pred: `fighten` :: Truth: `fighter`\n",
            "test img: Dic\n",
            "\n",
            "EPOCH: 64.000  validation_word_acc: 0.656  validation_char_acc: 0.833  train_char_acc: 0.969  validation_loss: 0.687  train_word_acc: 0.913  train_loss: 0.065  (17610.21s - 9905.74s remaining)\n",
            "\n",
            "Pred: `Mr.` :: Truth: `Mr.`\n",
            "Pred: `anyeyody` :: Truth: `anybody`\n",
            "Pred: `satellite` :: Truth: `satellite`\n",
            "Pred: `arrunged` :: Truth: `aroused`\n",
            "Pred: `whose` :: Truth: `whose`\n",
            "test img: pier\n",
            "\n",
            "EPOCH: 65.000  validation_word_acc: 0.813  validation_char_acc: 0.931  train_char_acc: 0.976  validation_loss: 0.314  train_word_acc: 0.925  train_loss: 0.068  (18019.77s - 9702.95s remaining)\n",
            "\n",
            "Pred: `of` :: Truth: `of`\n",
            "Pred: `start` :: Truth: `start`\n",
            "Pred: `heip` :: Truth: `help`\n",
            "Pred: `indeed` :: Truth: `indeed`\n",
            "Pred: `identify` :: Truth: `identify`\n",
            "test img: pie\n",
            "\n",
            "EPOCH: 66.000  validation_word_acc: 0.811  validation_char_acc: 0.929  train_char_acc: 0.970  validation_loss: 0.314  train_word_acc: 0.912  train_loss: 0.068  (18430.98s - 9494.75s remaining)\n",
            "\n",
            "Pred: `an` :: Truth: `an`\n",
            "Pred: `work` :: Truth: `work`\n",
            "Pred: `would` :: Truth: `would`\n",
            "Pred: `possible` :: Truth: `possible`\n",
            "Pred: `forget` :: Truth: `forget`\n",
            "test img: pie\n",
            "\n",
            "EPOCH: 67.000  validation_word_acc: 0.738  validation_char_acc: 0.887  train_char_acc: 0.975  validation_loss: 0.474  train_word_acc: 0.919  train_loss: 0.063  (18841.48s - 9280.13s remaining)\n",
            "\n",
            "Pred: `in` :: Truth: `in`\n",
            "Pred: `environment` :: Truth: `environment`\n",
            "Pred: `The` :: Truth: `The`\n",
            "Pred: `case` :: Truth: `case`\n",
            "Pred: `entire` :: Truth: `entire`\n",
            "test img: pies\n",
            "\n",
            "EPOCH: 68.000  validation_word_acc: 0.816  validation_char_acc: 0.931  train_char_acc: 0.976  validation_loss: 0.297  train_word_acc: 0.923  train_loss: 0.067  (19253.99s - 9060.70s remaining)\n",
            "\n",
            "Pred: `staff` :: Truth: `staff`\n",
            "Pred: `attack` :: Truth: `attack`\n",
            "Pred: `without` :: Truth: `without`\n",
            "Pred: `buids` :: Truth: `builds`\n",
            "Pred: `in` :: Truth: `in`\n",
            "test img: pie\n",
            "\n",
            "EPOCH: 69.000  validation_word_acc: 0.831  validation_char_acc: 0.938  train_char_acc: 0.975  validation_loss: 0.275  train_word_acc: 0.922  train_loss: 0.065  (19665.85s - 8835.38s remaining)\n",
            "\n",
            "Pred: `the` :: Truth: `the`\n",
            "Pred: `thing` :: Truth: `thing`\n",
            "Pred: `So` :: Truth: `So`\n",
            "Pred: `dream` :: Truth: `dream`\n",
            "Pred: `realize` :: Truth: `realize`\n",
            "test img: pier\n",
            "\n",
            "EPOCH: 70.000  validation_word_acc: 0.788  validation_char_acc: 0.905  train_char_acc: 0.977  validation_loss: 0.461  train_word_acc: 0.926  train_loss: 0.064  (20077.66s - 8604.71s remaining)\n",
            "\n",
            "Pred: `pay` :: Truth: `pay`\n",
            "Pred: `institution` :: Truth: `institution`\n",
            "Pred: `activity` :: Truth: `activity`\n",
            "Pred: `the` :: Truth: `the`\n",
            "Pred: `main` :: Truth: `main`\n",
            "test img: pies\n",
            "\n",
            "EPOCH: 71.000  validation_word_acc: 0.794  validation_char_acc: 0.920  train_char_acc: 0.972  validation_loss: 0.347  train_word_acc: 0.918  train_loss: 0.065  (20490.14s - 8369.21s remaining)\n",
            "\n",
            "Pred: `personal` :: Truth: `personal`\n",
            "Pred: `is` :: Truth: `is`\n",
            "Pred: `ol` :: Truth: `are`\n",
            "Pred: `around` :: Truth: `around`\n",
            "Pred: `the` :: Truth: `the`\n",
            "test img: fix\n",
            "\n",
            "EPOCH: 71.938  validation_loss: 0.447  validation_char_acc: 0.881  validation_word_acc: 0.594  (20899.36s - 8152.61s remaining)"
          ]
        }
      ],
      "source": [
        "from typing import ValuesView\n",
        "from torch.autograd.variable import Variable\n",
        "import shutil\n",
        "early_stop_patience=0\n",
        "ebochs=100\n",
        "lr=0.003\n",
        "optimizer=torch.optim.AdamW(model.parameters(),lr)\n",
        "# torch.save(trn_dataset,'drive/MyDrive/OCR(VGG19)/trn_dataset.data')\n",
        "# torch.save(val_dataset,'drive/MyDrive/OCR(VGG19)/val_dataset.data')\n",
        "log=Report(ebochs)\n",
        "N = len(trn_loader)\n",
        "V= len(val_loader)\n",
        "best_val_acc=0\n",
        "for eboch in range(21,ebochs):\n",
        "  for ix,data in enumerate(trn_loader):\n",
        "    model.train()\n",
        "    pos = eboch+(ix+1)/N\n",
        "    imgs,labels, label_len,targets, input_len = data\n",
        "    optimizer.zero_grad()\n",
        "    preds = model(imgs)\n",
        "    loss=ctc(preds, targets, input_len, label_len)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    eval_res=trn_dataset.evaluate(model,imgs,labels)\n",
        "    log.record(pos=pos, train_loss=loss, train_char_acc=eval_res['char-accuracy'],\n",
        "               train_word_acc=eval_res['word-accuracy'],end='\\r')\n",
        "\n",
        "  val_acc=0\n",
        "  with torch.no_grad():\n",
        "    for ix,data in enumerate(val_loader):\n",
        "      model.eval()\n",
        "      pos = eboch+(ix+1)/V\n",
        "      imgs,labels, label_len,targets, input_len = data\n",
        "      preds = model(imgs)\n",
        "      loss=ctc(preds, targets, input_len, label_len)\n",
        "      eval_res=val_dataset.evaluate(model,imgs,labels)\n",
        "      log.record(pos=pos, validation_loss=loss, validation_char_acc=eval_res['char-accuracy'],\n",
        "               validation_word_acc=eval_res['word-accuracy'], end='\\r')\n",
        "      val_acc+=eval_res['word-accuracy']\n",
        "    if val_acc>best_val_acc:\n",
        "      early_stop_patience=0\n",
        "      best_val_acc=val_acc\n",
        "      torch.save(model.state_dict(),'best.pt')\n",
        "      shutil.copy('best.pt','drive/MyDrive/OCR(VGG19)/best.pt')\n",
        "      print('saving best weights -- epoch {}'.format(eboch))\n",
        "      print()\n",
        "    else:\n",
        "      early_stop_patience+=1\n",
        "      if early_stop_patience==30:\n",
        "        print('Early stopping \\nsaving last weights...')\n",
        "        torch.save(model.state_dict(),'last.pt')\n",
        "    log.report_avgs(eboch+1)\n",
        "    print()\n",
        "    for jx in range(5):\n",
        "          img, label = val_dataset.sample()\n",
        "          _img = torch.Tensor(img).float().to(device)[None][None]\n",
        "          pred = model(_img)[:,0,:]\n",
        "          pred = val_dataset.decode(pred)\n",
        "          print(f'Pred: `{pred}` :: Truth: `{label}`')\n",
        "    imahm=cv2.imread('/content/img.jpg',0)\n",
        "    _imahm=torch.Tensor(imahm).float().to(device)[None,None]\n",
        "    pred = model(_imahm)[:,0,:]\n",
        "    pred = val_dataset.decode(pred)\n",
        "    print(\"test img: \"+pred)\n",
        "    if pred==\"fire\":\n",
        "      torch.save(model.state_dict(),'real_time.pt')\n",
        "    print()\n",
        "  torch.save(log,'log.log')\n",
        "  shutil.copy('log.log','drive/MyDrive/OCR(VGG19)/log.log')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}